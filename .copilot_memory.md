# COPILOT PROJECT MEMORY (DO NOT DELETE)

## Project Info
name: "Plataforma Preditiva - Backend Assíncrono"
repo_root: "/workspace/repo"

## Product Vision
Objetivo Final: Desenvolver um **Agente de IA para Automação da Cadeia de Suprimentos**.  
- Foco inicial: **Automação Inteligente de Ordens de Compra para Pequenas e Médias Indústrias**.  
- O sistema deve monitorar estoque, prever demanda, identificar melhores fornecedores e gerar pedidos automaticamente.  
- Público-alvo: **Profissionais de Compras** → o sistema apoia suas decisões, reduz trabalho manual e aumenta ROI.  
- Impacto esperado: Redução de custos, aumento de eficiência e prevenção de rupturas na produção.  

## Persona Context
role: "Desenvolvedor Python Sênior"
focus:
  - FastAPI
  - Celery + Redis
  - Docker Compose
  - SQLModel
  - MLOps básico (Prophet)
  - CrewAI
rules:
  - Sempre usar docker-compose com serviços api, db, broker
  - Sempre usar type hints em funções públicas
  - Retornar arquivos completos, não apenas snippets
  - Broker/Backend Redis: "redis://broker:6379/0"
  - Docker-compose: db = MySQL, volumes nomeados = postgres_data
  - Não avançar de fase sem validação `PASS`
  - Não adicionar dependências fora do Plano de Ação
  - Reescrever fases anteriores apenas se falharem

## Current Phase
phase: 4
status: "PENDING"

## Phase Checklist
0: PASS
1: PASS
2: PASS
3: PASS
4: PENDING
4.5: PENDING
5: PENDING

---

## Phase 0 — Estrutura inicial
- [x] Criar `.env` com placeholders (DATABASE_URL, REDIS_URL, MYSQL_ROOT_PASSWORD, etc.)
- [x] Criar `docker-compose.yml` básico (api, db, broker)
- [x] Criar `requirements.txt` ou `pyproject.toml` com dependências mínimas
- [x] Criar estrutura de pastas `app/core`, `app/models`, `app/routers`, `app/services`, `app/tasks`, `app/ml`, `scripts`
- [x] Validação: `tree -L 2` mostrando diretórios
- Status: PASS

---

## Phase 1 — API base + Celery + Docker
- [x] `docker-compose.yml` completo com db/mysql + broker/redis + volume postgres_data
- [x] `app/core/celery_app.py` criado
- [x] `app/tasks/debug_tasks.py` criado
- [x] `app/services/task_service.py` criado
- [x] `app/routers/tasks_router.py` criado
- [x] `app/main.py` criado (registrando rotas + startup hook)
- [x] Validação:
    command: |
      docker-compose up --build
      docker-compose exec api celery -A app.core.celery_app worker --loglevel=info
      curl -X POST http://localhost:8000/tasks/test
    expected_log_excerpt: "Task long_running_task[...] succeeded"
  result: PASS
- Status: PASS

---

## Phase 2 — Banco de Dados e Models
- [x] Criar `app/core/database.py` com engine lendo `DATABASE_URL` do .env
- [x] Criar `app/models/models.py` com modelos SQLModel:
      - Produto
      - VendasHistoricas
      - PrecosHistoricos
      - ModeloPredicao
- [x] Implementar `create_db_and_tables()` no `main.py`
- [x] Validar conexão e criação de tabelas:
      command: |
        docker compose exec api python -m compileall app
        docker compose exec db mysql -u app_user -papp_password -e "SHOW TABLES IN app_db;"
      expected_result:
        - CompileAll sem erros
        - Tabelas `produtos`, `vendas_historicas`, `precos_historicos`, `modelos_predicao` presentes
- Status: PASS

---

- [x] Criar `scripts/seed_database.py` (popular DB com CSV de vendas históricas)
- [x] Criar `app/ml/training.py` com função `train_prophet_model(produto_id: int) -> str` que salva modelo (ex: pickle/pth) e gera relatório básico em PDF
- [x] Criar `app/services/email_service.py` com função `send_training_report(report_data: TrainingReportData) -> None`
- [x] Criar `app/tasks/ml_tasks.py` com tarefa Celery:
      - `retrain_model_task(produto_id: int)` que:
        1. Roda `train_prophet_model`
        2. Gera relatório PDF
        3. Envia e-mail com PDF anexo e corpo estruturado para empresas
- [x] Criar rota `/vendas/upload` que dispara retraining por produto (e agenda a notificação)
- [x] Corpo de e-mail estruturado (exemplo):
      ```
      Assunto: Relatório de Re-Treino de Modelo — Produto X
      Prezados(as),

      Informamos que o modelo preditivo para o produto {{produto.nome}} foi atualizado com sucesso.

      • Produto: {{produto.nome}}
      • Data do treino: {{data}}
      • Precisão (MSE/RMSE): {{metricas}}
      • Próxima previsão disponível em anexo (PDF)

      Atenciosamente,
      Equipe de IA - Automação da Cadeia de Suprimentos
      ```
- [x] Validação:
    command: |
                  docker compose up -d --build api worker
                  curl -F "arquivo=@data/sample_vendas.csv" http://localhost:8000/vendas/upload
                  docker compose logs worker --tail=40
    expected_result: 
      - Worker processa tarefa `retrain_model_task`
                  - PDF gerado em `/app/artifacts/reports/{produto_id}_*.pdf`
                  - Log do e-mail enviado com sucesso
- Status: PASS

---

## Phase 4 — Scraping e Celery Beat (agendamento robusto e resiliente)
- [ ] Criar `app/scraping/scrapers.py` com funções de scraping usando **Playwright** (headless browser moderno, ideal para lidar com sites dinâmicos em JS).  
      - Fallback para `httpx` ou `BeautifulSoup4` quando o HTML for simples.  
      - Usar **rotating User-Agents** e **proxies** para evitar bloqueios.  
      - Implementar retries com **tenacity** (backoff exponencial).
- [ ] Criar `app/services/scraping_service.py` com função:
      - `scrape_and_save_price(produto_id: int) -> None`
      - Valida resposta, extrai preço, persiste no banco (`PrecosHistoricos`).
      - Loga sucesso ou erro com **structlog** (logs estruturados JSON).
- [ ] Criar `app/tasks/scraping_tasks.py` com tarefas Celery:
      - `scrape_product(produto_id: int)` — scrape individual.  
      - `scrape_all_products()` — itera todos os produtos ativos no DB.  
      - Garantir idempotência (mesmo produto não deve ser duplicado no mesmo horário).
- [ ] Adicionar serviço `beat` no docker-compose (celery beat) para agendamento.  
- [ ] Configurar `celery_app.conf.beat_schedule` para rodar scraping periodicamente:  
      - Exemplo: `scrape_all_products` a cada 8h.  
      - Usar **crontab** em vez de intervalos fixos (maior controle).
- [ ] Enviar notificações em caso de falha de scraping (opcional):  
      - Registrar erros críticos em `PrecosHistoricos` com flag de falha.  
      - integrar com e-mail  usando webhooks para alertar o time.
- [ ] Validação:
    command: |
      docker-compose up --build
      docker-compose exec api celery -A app.core.celery_app worker --loglevel=info
      docker-compose exec api celery -A app.core.celery_app beat --loglevel=info
    expected_result:
      - Worker executa `scrape_all_products` automaticamente no intervalo configurado
      - Logs estruturados mostrando scraping concluído
      - Novas entradas de preços salvas no DB (`PrecosHistoricos`)
- Status: PENDING


---

## Phase 4.5 — Integração com APIs Públicas de Localização e Fornecedores
- [ ] Criar `app/services/cep_service.py`:
      - Função `get_address_from_cep(cep: str) -> Dict[str, str]` usando **ViaCEP** (gratuito, sem API key).
- [ ] Criar `app/services/geolocation_service.py`:
      - Função `get_coordinates_from_address(address: str) -> Tuple[float, float]` usando **Nominatim (OpenStreetMap)**.
      - Função `calculate_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float` (fórmula de Haversine).
- [ ] Enriquecer `Fornecedor` no DB com:
      - CEP
      - Latitude/Longitude
- [ ] Criar lógica no serviço de pedidos:
      - Ao gerar ordem de compra, comparar fornecedores:
        - Pelo preço
        - Pelo prazo
        - Pela **proximidade geográfica** (distância).
      - Regra: em caso de **urgência**, priorizar fornecedor mais próximo.
- [ ] Criar endpoint `/fornecedores/enriquecer` para atualizar fornecedores com dados de CEP/geo.
- [ ] Observações:
      - Esses dados poderão ser usados no futuro:
        - Como **feature no modelo preditivo** (prazo de entrega estimado considerando distância).
        - Como **contexto no CrewAI** para análise multi-agente.
- [ ] Validação:
    - Seed inicial de fornecedores com CEPs.
    - Rodar enriquecimento (`/fornecedores/enriquecer`).
    - Gerar pedido com urgência → sistema deve escolher fornecedor mais próximo.
    - Logs devem mostrar cálculo de distância e fornecedor selecionado.
- Status: PENDING


---
## Phase 5 — CrewAI Agents + Deploy final
- [ ] Criar `app/agents/tools.py` (ferramentas que chamam API interna)
- [ ] Criar `app/agents/supply_chain_crew.py` (definição dos agentes e fluxo)
- [ ] Criar `app/services/crew_service.py` (função que monta e executa Crew)
- [ ] Ajustar `docker-compose.yml` para rodar `api`, `worker`, `beat`, `db`, `broker`
- [ ] Instruções de deploy (Render, Docker Swarm ou K8s)
- [ ] Validação: endpoint `/crew/execute-analysis` retorna execução real do Crew
- Status: PENDING
