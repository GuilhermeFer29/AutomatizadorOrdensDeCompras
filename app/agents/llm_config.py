"""
Configura√ß√£o Centralizada do LLM Gemini 2.5.

Este m√≥dulo fornece fun√ß√µes para configurar e retornar inst√¢ncias
dos modelos Gemini 2.5, garantindo consist√™ncia em todo o projeto e facilitando
manuten√ß√£o futura.

MODELOS DISPON√çVEIS (Google AI 2.5):
=====================================
‚Ä¢ gemini-2.5-flash: Alta performance, resposta r√°pida (PADR√ÉO)
‚Ä¢ gemini-2.5-pro: Racioc√≠nio avan√ßado, tarefas complexas

FUN√á√ïES PRINCIPAIS:
===================
‚Ä¢ get_gemini_llm(): Modelo padr√£o (Flash) configur√°vel
‚Ä¢ get_gemini_for_nlu(): Otimizado para NLU (temp=0.1)
‚Ä¢ get_gemini_for_creative(): Otimizado para criatividade (temp=0.7)
‚Ä¢ get_gemini_for_advanced_tasks(): Modelo PRO para tarefas complexas

Autor: Sistema de Automa√ß√£o de Compras
Atualiza√ß√£o: 2025-10-14 (Migra√ß√£o para Gemini 2.5)
"""

import os
from typing import Optional
from agno.models.google import Gemini


def get_gemini_llm(
    temperature: float = 0.3,
    model_id: str = "models/gemini-2.5-flash"
) -> Gemini:
    """
    Configura e retorna uma inst√¢ncia do modelo Google Gemini.
    
    Esta √© a √öNICA fun√ß√£o que deve ser usada para criar inst√¢ncias do LLM
    em todo o projeto. Isso garante:
    - Configura√ß√£o consistente em todos os agentes
    - Carregamento centralizado da API key
    - F√°cil manuten√ß√£o e atualiza√ß√£o de vers√µes
    - Valida√ß√£o adequada de vari√°veis de ambiente
    
    Args:
        temperature: Controla a aleatoriedade das respostas (0.0 = determin√≠stico, 1.0 = criativo).
                    Padr√£o: 0.3 (bom para tarefas anal√≠ticas)
        model_id: ID do modelo Gemini a ser usado.
                 Padr√£o: "models/gemini-2.5-flash" (mais recente e est√°vel)
                 Alternativas:
                 - "models/gemini-2.5-flash" (mais r√°pido, menos preciso)
                 - "models/gemini-2.5-pro" (vers√£o anterior)
    
    Returns:
        Gemini: Inst√¢ncia configurada do modelo Gemini pronta para uso.
    
    Raises:
        ValueError: Se a vari√°vel de ambiente GOOGLE_API_KEY n√£o estiver configurada.
    
    Example:
        ```python
        from app.agents.llm_config import get_gemini_llm
        from agno.agent import Agent
        
        # Obter modelo configurado
        llm = get_gemini_llm()
        
        # Usar em um agente
        agent = Agent(
            model=llm,
            instructions="Voc√™ √© um assistente especializado."
        )
        ```
    
    Notas:
        - A API key deve ser definida no arquivo .env: GOOGLE_API_KEY=sua_chave_aqui
        - Obtenha sua chave em: https://aistudio.google.com/app/apikey
        - O modelo Gemini 2.5 Flash suporta:
          * Contexto de at√© 1M tokens
          * Multimodal (texto, imagem, √°udio)
          * Function calling avan√ßado
          * JSON mode nativo
    """
    # Carrega API key do ambiente
    api_key = os.getenv("GOOGLE_API_KEY")
    
    # Valida√ß√£o cr√≠tica: sem API key = sem funcionamento
    if not api_key:
        raise ValueError(
            "‚ùå ERRO CR√çTICO: A vari√°vel de ambiente 'GOOGLE_API_KEY' n√£o est√° configurada.\n"
            "\n"
            "Por favor, adicione-a ao seu arquivo .env:\n"
            "  GOOGLE_API_KEY=sua_chave_aqui\n"
            "\n"
            "Obtenha sua chave gratuita em:\n"
            "  https://aistudio.google.com/app/apikey\n"
            "\n"
            "Ap√≥s adicionar, reinicie a aplica√ß√£o com:\n"
            "  docker-compose restart api worker\n"
        )
    
    # Log de configura√ß√£o (√∫til para debug)
    masked_key = f"{api_key[:8]}...{api_key[-4:]}" if len(api_key) > 12 else "***"
    print(f"ü§ñ Gemini LLM configurado: {model_id} (temp={temperature}, key={masked_key})")
    
    # Cria e retorna inst√¢ncia configurada
    return Gemini(
        id=model_id,
        api_key=api_key,
        temperature=temperature,
        # Configura√ß√µes adicionais (podem ser expandidas conforme necess√°rio)
        # max_tokens=None,  # Usa o m√°ximo do modelo
        # top_p=0.95,       # Nucleus sampling
        # top_k=40,         # Top-k sampling
    )


def get_gemini_for_nlu() -> Gemini:
    """
    Retorna uma inst√¢ncia do Gemini otimizada para tarefas de NLU (Natural Language Understanding).
    
    Configura√ß√£o especializada:
    - Temperature baixa (0.1) para respostas mais determin√≠sticas
    - Ideal para extra√ß√£o de entidades e classifica√ß√£o de inten√ß√µes
    
    Returns:
        Gemini: Inst√¢ncia otimizada para NLU.
    """
    return get_gemini_llm(temperature=0.1)


def get_gemini_for_creative() -> Gemini:
    """
    Retorna uma inst√¢ncia do Gemini otimizada para tarefas criativas.
    
    Configura√ß√£o especializada:
    - Temperature alta (0.7) para respostas mais variadas
    - Ideal para gera√ß√£o de relat√≥rios e an√°lises narrativas
    
    Returns:
        Gemini: Inst√¢ncia otimizada para cria√ß√£o de conte√∫do.
    """
    return get_gemini_llm(temperature=0.7)


def get_gemini_for_fast_agents() -> Gemini:
    """
    Retorna Gemini 2.5 Flash otimizado para agentes intermedi√°rios r√°pidos.
    
    Configura√ß√£o especializada:
    - Modelo: gemini-2.5-flash (velocidade m√°xima)
    - Temperature: 0.2 (determin√≠stico mas n√£o r√≠gido)
    - Ideal para: Agentes especialistas que precisam processar dados rapidamente
    
    Casos de uso:
    - AnalistaDemanda: An√°lise de estoque e previs√µes
    - PesquisadorMercado: Busca de ofertas e pre√ßos
    - AnalistaLogistica: C√°lculos de dist√¢ncia e prazos
    
    Returns:
        Gemini: Inst√¢ncia Flash otimizada para velocidade.
    
    Performance:
        - ~2-3x mais r√°pido que Pro
        - Custo reduzido
        - Precis√£o adequada para tarefas estruturadas
    """
    return get_gemini_llm(
        temperature=0.2,
        model_id="models/gemini-2.5-flash"
    )


def get_gemini_for_decision_making() -> Gemini:
    """
    Retorna Gemini 2.5 Pro otimizado para tomada de decis√µes cr√≠ticas.
    
    Configura√ß√£o especializada:
    - Modelo: gemini-2.5-pro (racioc√≠nio profundo)
    - Temperature: 0.1 (m√°xima precis√£o e consist√™ncia)
    - Ideal para: Decis√µes finais que impactam neg√≥cios
    
    Casos de uso:
    - GerenteCompras: Decis√£o final de aprovar/rejeitar compra
    - ConversationalAgent: Intera√ß√£o com usu√°rio (UX cr√≠tica)
    
    Returns:
        Gemini: Inst√¢ncia Pro otimizada para decis√µes.
    
    Performance:
        - Mais lento mas muito mais preciso
        - Melhor racioc√≠nio multi-etapas
        - Reduz erros em decis√µes cr√≠ticas
    """
    return get_gemini_llm(
        temperature=0.1,
        model_id="models/gemini-2.5-pro"
    )


def get_gemini_for_advanced_tasks() -> Gemini:
    """
    Retorna uma inst√¢ncia do Gemini otimizada para tarefas complexas e avan√ßadas.
    
    Configura√ß√£o especializada:
    - Modelo: gemini-2.5-pro-latest (m√°ximo poder de racioc√≠nio)
    - Temperature: 0.7 (criativo mas controlado)
    - Ideal para: An√°lises complexas, gera√ß√£o criativa, instru√ß√µes elaboradas
    
    Casos de uso:
    - An√°lise profunda de cadeia de suprimentos
    - Gera√ß√£o de relat√≥rios executivos
    - Tomada de decis√µes complexas
    - S√≠ntese de informa√ß√µes de m√∫ltiplas fontes
    
    Returns:
        Gemini: Inst√¢ncia do modelo PRO otimizada para tarefas avan√ßadas.
    
    Example:
        ```python
        from app.agents.llm_config import get_gemini_for_advanced_tasks
        from agno.agent import Agent
        
        # Agente para an√°lise complexa
        analyst = Agent(
            model=get_gemini_for_advanced_tasks(),
            instructions="Realize an√°lise profunda de dados de supply chain..."
        )
        ```
    
    Notas:
        - O modelo PRO √© mais lento mas significativamente mais preciso
        - Use apenas quando a tarefa realmente exigir racioc√≠nio avan√ßado
        - Para tarefas simples, use get_gemini_llm() (Flash √© mais r√°pido)
    """
    return get_gemini_llm(
        temperature=0.7,
        model_id="models/gemini-2.5-pro-latest"
    )


# Valida√ß√£o ao importar o m√≥dulo
if __name__ == "__main__":
    print("üß™ Testando configura√ß√£o do Gemini LLM...")
    
    try:
        llm = get_gemini_llm()
        print(f"‚úÖ Sucesso! Modelo configurado: {llm.id}")
        print(f"   Temperature: {llm.temperature}")
        print(f"   API Key: {'‚úì Configurada' if llm.api_key else '‚úó Ausente'}")
    except ValueError as e:
        print(f"‚ùå Erro: {e}")
    except Exception as e:
        print(f"‚ùå Erro inesperado: {e}")
